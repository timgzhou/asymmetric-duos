{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be6b821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating uncertainty visualization figures...\n",
      "1. Creating calibration plot...\n",
      "2. Creating CP-AUROC plot...\n",
      "3. Creating selective classification plot...\n",
      "\n",
      "All figures saved to 'uncertainty_figures/' directory\n",
      "Generated both PNG (for poster) and PDF (for high-quality printing)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for publication-quality figures\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['axes.linewidth'] = 1.5\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "GOOD_UNC_COLOR=\"#4d8ec3\"\n",
    "BAD_UNC_COLOR=\"#e49c30\"\n",
    "\n",
    "def create_cp_auroc_plot():\n",
    "    \"\"\"\n",
    "    Create uncertainty separation plot (correct vs incorrect predictions)\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    # Generate synthetic data for correct and incorrect predictions\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Good model: clear separation\n",
    "    correct_unc = np.random.beta(2, 5, 700) * 0.5  # Lower uncertainty\n",
    "    incorrect_unc = np.random.beta(4, 3, 300) * 0.6 + 0.1  # Higher uncertainty\n",
    "    \n",
    "    # Create histogram\n",
    "    bins = np.linspace(0, 1, 30)\n",
    "    ax.hist(correct_unc, bins=bins, alpha=0.7, label='Correct Predictions', \n",
    "            color=GOOD_UNC_COLOR, edgecolor='black', linewidth=1.2)\n",
    "    ax.hist(incorrect_unc, bins=bins, alpha=0.7, label='Incorrect Predictions', \n",
    "            color=BAD_UNC_COLOR, edgecolor='black', linewidth=1.2)\n",
    "    \n",
    "    ax.set_xlabel('Model Uncertainty (Softmax Response)', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('density', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Uncertainty Separability', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper right', fontsize=11, frameon=True, shadow=True)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--', axis='y')\n",
    "    \n",
    "    # Add separation indicator\n",
    "    ax.axvline(x=0.3, color='black', linestyle='--', linewidth=2, alpha=0.5)\n",
    "    ax.text(0.3, ax.get_ylim()[1] * 0.9, 'Decision\\nThreshold', \n",
    "            ha='center', fontsize=10, style='italic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def create_selective_classification_plot():\n",
    "    \"\"\"\n",
    "    Create selective classification curve (risk vs coverage) - AURC\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    # Coverage from 0 to 100%\n",
    "    coverage = np.linspace(0, 100, 100)\n",
    "    \n",
    "    # Base model accuracy\n",
    "    base_acc = 75\n",
    "    \n",
    "    # Model with good uncertainty: can improve accuracy by abstaining\n",
    "    good_unc_acc = base_acc + (100 - base_acc) * (1 - coverage/100)**0.5\n",
    "    \n",
    "    # Model with poor uncertainty: barely improves\n",
    "    poor_unc_acc = base_acc + (100 - base_acc) * (1 - coverage/100)**2.5\n",
    "    \n",
    "    # Convert to risk (100 - accuracy)\n",
    "    good_unc_risk = 100 - good_unc_acc\n",
    "    poor_unc_risk = 100 - poor_unc_acc\n",
    "    \n",
    "    # Random selection (oracle baseline)\n",
    "    random_risk = np.ones_like(coverage) * (100 - base_acc)\n",
    "    \n",
    "    ax.plot(coverage, good_unc_risk, linewidth=3, label='Good Uncertainty', \n",
    "            color=GOOD_UNC_COLOR, marker='o', markevery=10, markersize=6)\n",
    "    ax.plot(coverage, poor_unc_risk, linewidth=3, label='Poor Uncertainty', \n",
    "            color=BAD_UNC_COLOR, marker='s', markevery=10, markersize=6)\n",
    "    ax.plot(coverage, random_risk, 'k--', linewidth=2, label='No Selection (Baseline)', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Coverage (% of predictions made)', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Risk (% error)', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Risk-Coverage Tradeoff', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='lower right', fontsize=11, frameon=False, shadow=False)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.set_xlim(0, 110)\n",
    "    ax.set_ylim(0, 30)\n",
    "    \n",
    "    # Highlight area between curves - gain in AURC\n",
    "    ax.fill_between(coverage, good_unc_risk, poor_unc_risk, alpha=0.2, color='gold')\n",
    "    ax.text(50, 15, 'Gain in AURC', ha='center', fontsize=10, \n",
    "            style='italic', color='darkgoldenrod', fontweight='bold')\n",
    "    \n",
    "    # Label error rate at coverage=100\n",
    "    error_rate = 100 - base_acc\n",
    "    ax.plot(100, error_rate, 'ko', markersize=8, zorder=5)\n",
    "    ax.text(105, error_rate + 1, f'Error Rate\\n({error_rate:.0f}%)', ha='right', fontsize=10, \n",
    "            fontweight='bold')\n",
    "    \n",
    "    # Add horizontal line at risk=0.5 to show SAC@95 gain\n",
    "    risk_threshold = 5\n",
    "    # Find coverage where each curve reaches this risk level\n",
    "    idx_good = np.argmin(np.abs(good_unc_risk - risk_threshold))\n",
    "    idx_poor = np.argmin(np.abs(poor_unc_risk - risk_threshold))\n",
    "    cov_good = coverage[idx_good]\n",
    "    cov_poor = coverage[idx_poor]\n",
    "    \n",
    "    # Draw horizontal line between the two curves at this risk level\n",
    "    ax.plot([cov_poor, cov_good], [risk_threshold, risk_threshold], \n",
    "            '-', linewidth=2.5, alpha=0.7, zorder=2,color=\"gray\")\n",
    "    ax.plot([cov_poor, cov_good], [risk_threshold, risk_threshold], \n",
    "            'o', markersize=6, zorder=2,color=\"gray\")\n",
    "    \n",
    "    # Label the gain in SAC@95\n",
    "    mid_cov = (cov_good + cov_poor) / 2\n",
    "    ax.text(mid_cov+0.8, risk_threshold + 0.4, f'Gain in SAC@{risk_threshold:.1f}', \n",
    "            ha='center', fontsize=9, fontweight='bold', color='gray')\n",
    "    \n",
    "    # Add annotation\n",
    "    ax.text(40, 27, 'Abstain on uncertain examples â†’ lower risk',\n",
    "            fontsize=11, ha='center', style='italic',\n",
    "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Generate all figures for the poster\n",
    "    \"\"\"\n",
    "    print(\"Generating uncertainty visualization figures...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    import os\n",
    "    os.makedirs('uncertainty_figures', exist_ok=True)\n",
    "\n",
    "    print(\"1. Creating calibration plot...\")\n",
    "    fig1 = create_calibration_plot()\n",
    "    fig1.savefig('uncertainty_figures/calibration.png', dpi=300, bbox_inches='tight')\n",
    "    fig1.savefig('uncertainty_figures/calibration.pdf', bbox_inches='tight')\n",
    "    plt.close(fig1)\n",
    "\n",
    "    print(\"2. Creating CP-AUROC plot...\")\n",
    "    fig2 = create_cp_auroc_plot()\n",
    "    fig2.savefig('uncertainty_figures/cp_auroc.png', dpi=300, bbox_inches='tight')\n",
    "    fig2.savefig('uncertainty_figures/cp_auroc.pdf', bbox_inches='tight')\n",
    "    plt.close(fig2)\n",
    "    \n",
    "    print(\"3. Creating selective classification plot...\")\n",
    "    fig3 = create_selective_classification_plot()\n",
    "    fig3.savefig('uncertainty_figures/selective_classification.png', dpi=300, bbox_inches='tight')\n",
    "    fig3.savefig('uncertainty_figures/selective_classification.pdf', bbox_inches='tight')\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    print(\"\\nAll figures saved to 'uncertainty_figures/' directory\")\n",
    "    print(\"Generated both PNG (for poster) and PDF (for high-quality printing)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "gfv5d3vp3j",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_calibration_plot():\n",
    "    \"\"\"\n",
    "    Create calibration plot comparing well-calibrated vs overconfident model\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    \n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create 10 bins\n",
    "    n_bins = 10\n",
    "    bin_edges = np.linspace(0, 1, n_bins + 1)\n",
    "    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "    \n",
    "    # Well-calibrated model: predicted confidence matches actual accuracy with some noise\n",
    "    calibrated_confidence = bin_centers\n",
    "    calibrated_accuracy = bin_centers + np.random.normal(0, 0.03, n_bins)  # Random noise\n",
    "    calibrated_accuracy = np.clip(calibrated_accuracy, 0, 1)\n",
    "    \n",
    "    # Overconfident model: actual accuracy is always lower than predicted confidence with noise\n",
    "    overconfident_confidence = bin_centers\n",
    "    # Create systematic overconfidence - worse at higher confidence\n",
    "    overconfidence_gap = 0.05 + 0.3 * bin_centers  # Gap increases with confidence\n",
    "    overconfident_accuracy = bin_centers - overconfidence_gap + np.random.normal(0, 0.05, n_bins)\n",
    "    overconfident_accuracy = np.clip(overconfident_accuracy, 0, 1)\n",
    "    \n",
    "    # Plot perfect calibration line first (so it's in the background)\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Perfect Calibration', alpha=0.7, zorder=1)\n",
    "    \n",
    "    # Plot calibrated model with transparency\n",
    "    ax.plot(calibrated_confidence, calibrated_accuracy, linewidth=3, \n",
    "            label='Calibrated', color=GOOD_UNC_COLOR, marker='o', markersize=8, \n",
    "            alpha=0.8, zorder=3)\n",
    "    \n",
    "    # Plot overconfident model with transparency\n",
    "    ax.plot(overconfident_confidence, overconfident_accuracy, linewidth=3, \n",
    "            label='Overconfident', color=BAD_UNC_COLOR, marker='s', markersize=8, \n",
    "            alpha=0.8, zorder=2)\n",
    "    \n",
    "    ax.set_xlabel('Predicted Confidence', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylabel('Actual Accuracy', fontsize=14, fontweight='bold')\n",
    "    ax.set_title('Calibration', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.legend(loc='upper left', fontsize=11, frameon=True, shadow=True)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2d9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ffadf3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
